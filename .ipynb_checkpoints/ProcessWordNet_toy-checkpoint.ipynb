{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "relation_feat_syn = None\n",
    "relation_feat_ant = None\n",
    "relation_feat_hype = None\n",
    "relation_feat_hypo = None\n",
    "relation_feat_same_hype = None\n",
    "def PrintRelationLst():\n",
    "    print('relation-syn:',relation_feat_syn)\n",
    "    print('relation-ant:',relation_feat_ant)\n",
    "    print('relation-hype:',relation_feat_hype)\n",
    "    print('relation-hypo:',relation_feat_hypo)\n",
    "    print('relation-same_hype:',relation_feat_same_hype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonymy & Antonymy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation-syn: 0\n",
      "relation-ant: 1\n",
      "relation-hype: None\n",
      "relation-hypo: None\n",
      "relation-same_hype: None\n"
     ]
    }
   ],
   "source": [
    "# WORD1 = 'good'\n",
    "# WORD2 = 'beneficial'\n",
    "# WORD1 = 'wet'\n",
    "# WORD2 = 'dry'\n",
    "WORD1 = 'wet'\n",
    "WORD2 = 'dry'\n",
    "syn_flg = False\n",
    "WORD1_syn_lst = wn.synsets(WORD1)\n",
    "WORD2_syn_lst =wn.synsets(WORD2)\n",
    "# synonymy\n",
    "for word1_i, word1_syn in enumerate(WORD1_syn_lst):\n",
    "    for word2_i, word2_syn in enumerate(WORD2_syn_lst):\n",
    "        if word1_syn == word2_syn:\n",
    "            relation_feat_syn = 1\n",
    "    \n",
    "# antonymy\n",
    "syn_flg = True\n",
    "for word1_i, word1_syn in enumerate(WORD1_syn_lst):\n",
    "    for word2_i, word2_syn in enumerate(WORD2_syn_lst):\n",
    "        if word1_syn.lemmas()[0].antonyms():\n",
    "#             print('word1:',word1_syn.lemmas()[0])\n",
    "#             print('word2:',word2_syn.lemmas()[0])\n",
    "#             print('============================')\n",
    "            if word1_syn.lemmas()[0].antonyms()[0] == word2_syn.lemmas()[0]:\n",
    "                relation_feat_ant = 1\n",
    "if not relation_feat_syn:\n",
    "    relation_feat_syn = 0\n",
    "if not relation_feat_ant:\n",
    "    relation_feat_ant = 0\n",
    "PrintRelationLst()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypernymy & Hpyponymy & Same-Hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1 =  wet\n",
      "word2 =  dry\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "# here due to its external knowledge definition, we will simple consider Hypernymy and Hyponymy as the same value\n",
    "# =========== def ===========\n",
    "# takes value 1 - n/8 if one word is a (direct or indirect) hypernym of the other wors in WordNet. \n",
    "# Where n is the number of edges between the two words in hierarchies, and 0 otherwise.\n",
    "# Eg. [dog, canid] = 0.875\n",
    "# Eg. [wolf, canid] = 0.875\n",
    "# Eg. [dog, carnivore] = 0.75\n",
    "# Eg. [canid, dog] = 0\n",
    "# ======== End def ==========\n",
    "\n",
    "## first find their most similarity def\n",
    "print('word1 = ',WORD1)\n",
    "print('word2 = ',WORD2)\n",
    "print('================')\n",
    "similarity = -np.float('inf')\n",
    "for word1_i, word1_syn in enumerate(WORD1_syn_lst):\n",
    "    for word2_i, word2_syn in enumerate(WORD2_syn_lst):\n",
    "        sim_val = wn.wup_similarity(word1_syn, word2_syn)\n",
    "        if sim_val: # if they exist similaroty value\n",
    "            if sim_val > similarity:\n",
    "                best_val = sim_val\n",
    "                best_syn1 = word1_syn\n",
    "                best_syn2 = word2_syn\n",
    "                similarity = sim_val\n",
    "# print(best_syn1)\n",
    "# print(best_syn2)\n",
    "        \n",
    "## base on the def, to figure out their hyper-path\n",
    "best_syn1_path = best_syn1.hypernym_paths()\n",
    "for idx, path in enumerate(best_syn1_path):\n",
    "    for i, hyper in enumerate(path):\n",
    "        if hyper == best_syn2:\n",
    "            edgN = len(path)-i-1\n",
    "            relation_feat_hype = 1-edgN/8\n",
    "            break\n",
    "    if relation_feat_hype:\n",
    "        break\n",
    "    \n",
    "best_syn2_path = best_syn2.hypernym_paths()\n",
    "for idx, path in enumerate(best_syn2_path):\n",
    "    for i, hyper in enumerate(path):\n",
    "        if hyper == best_syn1:\n",
    "            edgN = len(path)-i-1\n",
    "            relation_feat_hypo = 1-edgN/8\n",
    "            break\n",
    "    if relation_feat_hypo:\n",
    "        break\n",
    "\n",
    "if relation_feat_hype:\n",
    "    relation_feat_hypo = relation_feat_hype\n",
    "elif relation_feat_hypo:\n",
    "    relation_feat_hype = relation_feat_hypo\n",
    "else:\n",
    "    relation_feat_hype = 0\n",
    "    relation_feat_hypo = 0\n",
    "    \n",
    "## Same-Hypernym\n",
    "if relation_feat_syn == 0:\n",
    "    # to see if the two words have the same hypernym\n",
    "    for syn1_idx, syn1_path in enumerate(best_syn1_path):\n",
    "        for syn2_idx, syn2_path in enumerate(best_syn2_path):\n",
    "            for syn1_hyper in syn1_path:\n",
    "                for syn2_hyper in syn2_path:\n",
    "                    if syn1_hyper == syn2_hyper:\n",
    "#                         print(syn1_hyper)\n",
    "#                         print(syn2_hyper)\n",
    "                        relation_feat_same_hype = 1\n",
    "if not relation_feat_same_hype:\n",
    "    relation_feat_same_hype = 0\n",
    "# PrintRelationLst() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation-syn: 0\n",
      "relation-ant: 1\n",
      "relation-hype: 0\n",
      "relation-hypo: 0\n",
      "relation-same_hype: 0\n"
     ]
    }
   ],
   "source": [
    "PrintRelationLst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## External knowledge\n",
    "'''\n",
    "rij = [syn, ant, hype, hypo, same-hype]\n",
    "λ1(rij ), where λ is a hyper-parameter tuned on the development set \n",
    "and 1 is the indication function.\n",
    "1(rij ) = 1 if rij is not zero vector ;\n",
    "1(rij ) = 0 if rij is zero vector .\n",
    "'''\n",
    "r = [relation_feat_syn, relation_feat_ant, relation_feat_hype, relation_feat_hypo, relation_feat_same_hype] # r_ij\n",
    "if all(ele == 0 for ele in r):\n",
    "    lr = 0 # lr_ij\n",
    "else:\n",
    "    lr = 1 # lr_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from SNLI-dataset to build a relation-feature dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "import json\n",
    "import random    \n",
    "import string\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"contradiction\": 2,\n",
    "    \"hidden\": 0\n",
    "}\n",
    "\n",
    "def load_nli_data(path, snli=False):\n",
    "    \"\"\"\n",
    "    Load MultiNLI or SNLI data.\n",
    "    If the \"snli\" parameter is set to True, a genre label of snli will be assigned to the data. \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            loaded_example = json.loads(line)\n",
    "            if loaded_example[\"gold_label\"] not in LABEL_MAP:\n",
    "                continue\n",
    "            loaded_example[\"label\"] = LABEL_MAP[loaded_example[\"gold_label\"]]\n",
    "            if snli:\n",
    "                loaded_example[\"genre\"] = \"snli\"\n",
    "            data.append(loaded_example)\n",
    "        random.seed(1)\n",
    "        random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "test_snli = load_nli_data(\"./data/snli_1.0/snli_1.0_dev.jsonl\", snli=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process to 0th sentences.\n",
      "Process to 1th sentences.\n",
      "Process to 2th sentences.\n",
      "Process to 3th sentences.\n",
      "Process to 4th sentences.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-ad05943dab2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m#             print('process pair')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#             if word1 in Possible_words and word2 in Possible_words:  # only consider Noun/Verb/Adj/Adv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mR_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelation_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# five relation feature value: [ayn, ant, hype, hypo, same-hype]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mR_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0ml_R\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# lr_ij\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-ad05943dab2d>\u001b[0m in \u001b[0;36mrelation_feat\u001b[0;34m(WORD1, WORD2)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# hypernymy&hyponymy-prepare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m## first find their most similarity def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0msim_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwup_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1_syn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2_syn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msim_val\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# if they exist similaroty value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0msim_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mwup_similarity\u001b[0;34m(self, synset1, synset2, verbose, simulate_root)\u001b[0m\n\u001b[1;32m   1699\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulate_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m     ):\n\u001b[0;32m-> 1701\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msynset1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwup_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulate_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1702\u001b[0m     \u001b[0mwup_similarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwup_similarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mwup_similarity\u001b[0;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \"\"\"\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mneed_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_needs_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m         \u001b[0;31m# Note that to preserve behavior from NLTK2 we set use_min_depth=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;31m# It is possible that more accurate results could be obtained by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_needs_root\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_needs_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mNOUN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wordnet_corpus_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1.6'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mget_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mADJ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m             \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'WordNet (\\d+\\.\\d+) Copyright'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0;34m\"\"\"Return the next decoded line from the underlying stream.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinebuffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0mchars\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinebuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinebuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def relation_feat(WORD1, WORD2):\n",
    "    # init\n",
    "    relation_feat_syn = None\n",
    "    relation_feat_ant = None\n",
    "    relation_feat_hype = None\n",
    "    relation_feat_hypo = None\n",
    "    relation_feat_same_hype = None\n",
    "    \n",
    "    syn_flg = False\n",
    "    WORD1_syn_lst = wn.synsets(WORD1)\n",
    "    WORD2_syn_lst =wn.synsets(WORD2)\n",
    "    if not WORD1_syn_lst or not WORD2_syn_lst:\n",
    "        return [0,0,0,0,0]\n",
    "    \n",
    "    similarity = -np.float('inf')\n",
    "    sim_flag = False\n",
    "    for word1_i, word1_syn in enumerate(WORD1_syn_lst):\n",
    "        for word2_i, word2_syn in enumerate(WORD2_syn_lst):\n",
    "            # synonymy\n",
    "            if word1_syn == word2_syn:\n",
    "                relation_feat_syn = 1\n",
    "            # antonymy\n",
    "            if word1_syn.lemmas()[0].antonyms():\n",
    "                if word1_syn.lemmas()[0].antonyms()[0] == word2_syn.lemmas()[0]:\n",
    "                    relation_feat_ant = 1\n",
    "            # hypernymy&hyponymy-prepare\n",
    "            ## first find their most similarity def\n",
    "            sim_val = wn.wup_similarity(word1_syn, word2_syn)\n",
    "            if sim_val: # if they exist similaroty value\n",
    "                sim_flag = True\n",
    "                if sim_val > similarity:\n",
    "                    best_val = sim_val\n",
    "                    best_syn1 = word1_syn\n",
    "                    best_syn2 = word2_syn\n",
    "                    similarity = sim_val\n",
    "    \n",
    "    if not relation_feat_syn:\n",
    "        relation_feat_syn = 0\n",
    "    if not relation_feat_ant:\n",
    "        relation_feat_ant = 0     \n",
    "    if not sim_flag: # in case the 2 words don't exist any similarity definision synset.\n",
    "        return [0,0,0,0,0]\n",
    "    \n",
    "    # hypernymy&hyponymy\n",
    "    edgN = best_syn1.shortest_path_distance(best_syn2)\n",
    "    if edgN:\n",
    "        relation_feat_hype = 1-edgN/8\n",
    "        relation_feat_hypo = 1-edgN/8\n",
    "    else:\n",
    "        relation_feat_hype = 0\n",
    "        relation_feat_hypo = 0\n",
    "    \n",
    "    # same-hypernymy\n",
    "    if best_syn1.wup_similarity(best_syn2): #  wup_similarity will return None, when the two synsets have no common hypernym\n",
    "        relation_feat_same_hype = 1\n",
    "    else:\n",
    "        relation_feat_same_hype = 0\n",
    "        \n",
    "        \n",
    "    return [relation_feat_syn, relation_feat_ant, relation_feat_hype, relation_feat_hypo, relation_feat_same_hype]\n",
    "\n",
    "# remove all punctuations (except while space)\n",
    "def preprocess_word(str_word):\n",
    "    signtext = string.punctuation \n",
    "    signrepl = '@'*len(signtext)\n",
    "    signtable = str_word.maketrans(signtext,signrepl) \n",
    "    return str_word.translate(signtable).replace('@','')\n",
    "\n",
    "Dic_external_know = {}\n",
    "for data_i, data_ in enumerate(test_snli):\n",
    "    print('Process to {}th sentences.'.format(data_i))\n",
    "    sent_1 = data_['sentence1']\n",
    "    sent_2 = data_['sentence2'] \n",
    "    sent_1 = preprocess_word(sent_1) # remove all punctuations\n",
    "    sent_2 = preprocess_word(sent_2) # remove all punctuations\n",
    "    for word1 in sent_1.split():\n",
    "        for word2 in sent_2.split():\n",
    "            if (word1,word2) in Dic_external_know:\n",
    "                continue\n",
    "            else:\n",
    "    #             print('process pair')\n",
    "    #             if word1 in Possible_words and word2 in Possible_words:  # only consider Noun/Verb/Adj/Adv\n",
    "                R_lst = relation_feat(word1, word2) # five relation feature value: [ayn, ant, hype, hypo, same-hype]\n",
    "                if all(ele == 0 for ele in R_lst):\n",
    "                    l_R = 0 # lr_ij\n",
    "                else:\n",
    "                    l_R = 1 # lr_ij\n",
    "                Dic_external_know[(word1,word2)] = l_R # key: (word1, word2); value: L(R_ij) [should be 1 or 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('snli_dev_relationship_feat.pkl', 'wb')\n",
    "pickle.dump(Dic_external_know, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ProcessWordNet_toy.ipynb to script\n",
      "[NbConvertApp] Writing 9818 bytes to ProcessWordNet_toy.py\n"
     ]
    }
   ],
   "source": [
    "# Dic_external_know\n",
    "!jupyter nbconvert --to script ProcessWordNet_toy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns2 = {x.name().split('.', 1)[0] for x in wn.all_synsets(wn.NOUN)}\n",
    "verbs = {x.name().split('.', 1)[0] for x in wn.all_synsets(wn.VERB)}\n",
    "adjs = {x.name().split('.', 1)[0] for x in wn.all_synsets(wn.ADJ)}\n",
    "advs = {x.name().split('.', 1)[0] for x in wn.all_synsets(wn.ADV)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3050"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(nouns)\n",
    "len(nouns2)\n",
    "len(verbs)\n",
    "len(adjs)\n",
    "len(advs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7440\n",
      "3050\n",
      "10442\n"
     ]
    }
   ],
   "source": [
    "type(adjs)\n",
    "# print(verbs)\n",
    "print(len(verbs))\n",
    "# print(advs)\n",
    "print(len(advs))\n",
    "# print(verbs.union(advs))\n",
    "print(len(verbs.union(advs)))\n",
    "Possible_words = nouns.union(verbs).union(adjs).union(advs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[[Synset('entity.n.01'), Synset('abstraction.n.06'), Synset('psychological_feature.n.01'), Synset('cognition.n.01'), Synset('content.n.05'), Synset('belief.n.01'), Synset('spiritual_being.n.01'), Synset('deity.n.01'), Synset('greek_deity.n.01'), Synset('titan.n.02')]]\n",
      "[[Synset('express.v.02'), Synset('state.v.01')]]\n"
     ]
    }
   ],
   "source": [
    "t = wn.synsets('titan')[1]\n",
    "s = wn.synsets('say', wn.VERB)[0]\n",
    "print(t.wup_similarity(s))\n",
    "print(wn.synsets('titan')[1].hypernym_paths())\n",
    "print(wn.synsets('say', wn.VERB)[0].hypernym_paths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.631578947368421\n",
      "[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('causal_agent.n.01'), Synset('person.n.01'), Synset('creator.n.02'), Synset('artist.n.01'), Synset('painter.n.01'), Synset('picasso.n.01')], [Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('living_thing.n.01'), Synset('organism.n.01'), Synset('person.n.01'), Synset('creator.n.02'), Synset('artist.n.01'), Synset('painter.n.01'), Synset('picasso.n.01')], [Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('causal_agent.n.01'), Synset('person.n.01'), Synset('creator.n.02'), Synset('artist.n.01'), Synset('sculptor.n.01'), Synset('picasso.n.01')], [Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('living_thing.n.01'), Synset('organism.n.01'), Synset('person.n.01'), Synset('creator.n.02'), Synset('artist.n.01'), Synset('sculptor.n.01'), Synset('picasso.n.01')]]\n",
      "[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('causal_agent.n.01'), Synset('person.n.01'), Synset('male.n.02')], [Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('living_thing.n.01'), Synset('organism.n.01'), Synset('person.n.01'), Synset('male.n.02')]]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "t = wn.synsets('picasso')[0]\n",
    "m = wn.synsets('male')[1]\n",
    "print(t.wup_similarity(m))\n",
    "print(wn.synsets('picasso')[0].hypernym_paths())\n",
    "print(wn.synsets('male')[1].hypernym_paths())\n",
    "print(t.shortest_path_distance(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
