{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "relation_feat_syn = None\n",
    "relation_feat_ant = None\n",
    "relation_feat_hype = None\n",
    "relation_feat_hypo = None\n",
    "relation_feat_same_hype = None\n",
    "def PrintRelationLst():\n",
    "    print('relation-syn:',relation_feat_syn)\n",
    "    print('relation-ant:',relation_feat_ant)\n",
    "    print('relation-hype:',relation_feat_hype)\n",
    "    print('relation-hypo:',relation_feat_hypo)\n",
    "    print('relation-same_hype:',relation_feat_same_hype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonymy & Antonymy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation-syn: 0\n",
      "relation-ant: 1\n",
      "relation-hype: None\n",
      "relation-hypo: None\n",
      "relation-same_hype: None\n"
     ]
    }
   ],
   "source": [
    "# WORD1 = 'good'\n",
    "# WORD2 = 'beneficial'\n",
    "# WORD1 = 'wet'\n",
    "# WORD2 = 'dry'\n",
    "WORD1 = 'wet'\n",
    "WORD2 = 'dry'\n",
    "syn_flg = False\n",
    "WORD1_syn_lst = wn.synsets(WORD1)\n",
    "WORD2_syn_lst =wn.synsets(WORD2)\n",
    "# synonymy\n",
    "for word1_i, word1_syn in enumerate(WORD1_syn_lst):\n",
    "    for word2_i, word2_syn in enumerate(WORD2_syn_lst):\n",
    "        if word1_syn == word2_syn:\n",
    "            relation_feat_syn = 1\n",
    "    \n",
    "# antonymy\n",
    "syn_flg = True\n",
    "for word1_i, word1_syn in enumerate(WORD1_syn_lst):\n",
    "    for word2_i, word2_syn in enumerate(WORD2_syn_lst):\n",
    "        if word1_syn.lemmas()[0].antonyms():\n",
    "#             print('word1:',word1_syn.lemmas()[0])\n",
    "#             print('word2:',word2_syn.lemmas()[0])\n",
    "#             print('============================')\n",
    "            if word1_syn.lemmas()[0].antonyms()[0] == word2_syn.lemmas()[0]:\n",
    "                relation_feat_ant = 1\n",
    "if not relation_feat_syn:\n",
    "    relation_feat_syn = 0\n",
    "if not relation_feat_ant:\n",
    "    relation_feat_ant = 0\n",
    "PrintRelationLst()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypernymy & Hpyponymy & Same-Hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1 =  wet\n",
      "word2 =  dry\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "# here due to its external knowledge definition, we will simple consider Hypernymy and Hyponymy as the same value\n",
    "# =========== def ===========\n",
    "# takes value 1 - n/8 if one word is a (direct or indirect) hypernym of the other wors in WordNet. \n",
    "# Where n is the number of edges between the two words in hierarchies, and 0 otherwise.\n",
    "# Eg. [dog, canid] = 0.875\n",
    "# Eg. [wolf, canid] = 0.875\n",
    "# Eg. [dog, carnivore] = 0.75\n",
    "# Eg. [canid, dog] = 0\n",
    "# ======== End def ==========\n",
    "\n",
    "## first find their most similarity def\n",
    "print('word1 = ',WORD1)\n",
    "print('word2 = ',WORD2)\n",
    "print('================')\n",
    "similarity = -np.float('inf')\n",
    "for word1_i, word1_syn in enumerate(WORD1_syn_lst):\n",
    "    for word2_i, word2_syn in enumerate(WORD2_syn_lst):\n",
    "        sim_val = wn.wup_similarity(word1_syn, word2_syn)\n",
    "        if sim_val: # if they exist similaroty value\n",
    "            if sim_val > similarity:\n",
    "                best_val = sim_val\n",
    "                best_syn1 = word1_syn\n",
    "                best_syn2 = word2_syn\n",
    "                similarity = sim_val\n",
    "# print(best_syn1)\n",
    "# print(best_syn2)\n",
    "        \n",
    "## base on the def, to figure out their hyper-path\n",
    "best_syn1_path = best_syn1.hypernym_paths()\n",
    "for idx, path in enumerate(best_syn1_path):\n",
    "    for i, hyper in enumerate(path):\n",
    "        if hyper == best_syn2:\n",
    "            edgN = len(path)-i-1\n",
    "            relation_feat_hype = 1-edgN/8\n",
    "            break\n",
    "    if relation_feat_hype:\n",
    "        break\n",
    "    \n",
    "best_syn2_path = best_syn2.hypernym_paths()\n",
    "for idx, path in enumerate(best_syn2_path):\n",
    "    for i, hyper in enumerate(path):\n",
    "        if hyper == best_syn1:\n",
    "            edgN = len(path)-i-1\n",
    "            relation_feat_hypo = 1-edgN/8\n",
    "            break\n",
    "    if relation_feat_hypo:\n",
    "        break\n",
    "\n",
    "if relation_feat_hype:\n",
    "    relation_feat_hypo = relation_feat_hype\n",
    "elif relation_feat_hypo:\n",
    "    relation_feat_hype = relation_feat_hypo\n",
    "else:\n",
    "    relation_feat_hype = 0\n",
    "    relation_feat_hypo = 0\n",
    "    \n",
    "## Same-Hypernym\n",
    "if relation_feat_syn == 0:\n",
    "    # to see if the two words have the same hypernym\n",
    "    for syn1_idx, syn1_path in enumerate(best_syn1_path):\n",
    "        for syn2_idx, syn2_path in enumerate(best_syn2_path):\n",
    "            for syn1_hyper in syn1_path:\n",
    "                for syn2_hyper in syn2_path:\n",
    "                    if syn1_hyper == syn2_hyper:\n",
    "#                         print(syn1_hyper)\n",
    "#                         print(syn2_hyper)\n",
    "                        relation_feat_same_hype = 1\n",
    "if not relation_feat_same_hype:\n",
    "    relation_feat_same_hype = 0\n",
    "# PrintRelationLst() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation-syn: 0\n",
      "relation-ant: 1\n",
      "relation-hype: 0\n",
      "relation-hypo: 0\n",
      "relation-same_hype: 0\n"
     ]
    }
   ],
   "source": [
    "PrintRelationLst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## External knowledge\n",
    "'''\n",
    "rij = [syn, ant, hype, hypo, same-hype]\n",
    "λ1(rij ), where λ is a hyper-parameter tuned on the development set \n",
    "and 1 is the indication function.\n",
    "1(rij ) = 1 if rij is not zero vector ;\n",
    "1(rij ) = 0 if rij is zero vector .\n",
    "'''\n",
    "r = [relation_feat_syn, relation_feat_ant, relation_feat_hype, relation_feat_hypo, relation_feat_same_hype] # r_ij\n",
    "if all(ele == 0 for ele in r):\n",
    "    lr = 0 # lr_ij\n",
    "else:\n",
    "    lr = 1 # lr_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from SNLI-dataset to build a relation-feature dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "import json\n",
    "import random    \n",
    "import string\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"contradiction\": 2,\n",
    "    \"hidden\": 0\n",
    "}\n",
    "\n",
    "def load_nli_data(path, snli=False):\n",
    "    \"\"\"\n",
    "    Load MultiNLI or SNLI data.\n",
    "    If the \"snli\" parameter is set to True, a genre label of snli will be assigned to the data. \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            loaded_example = json.loads(line)\n",
    "            if loaded_example[\"gold_label\"] not in LABEL_MAP:\n",
    "                continue\n",
    "            loaded_example[\"label\"] = LABEL_MAP[loaded_example[\"gold_label\"]]\n",
    "            if snli:\n",
    "                loaded_example[\"genre\"] = \"snli\"\n",
    "            data.append(loaded_example)\n",
    "        random.seed(1)\n",
    "        random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "test_snli = load_nli_data(\"./data/snli_1.0/snli_1.0_test.jsonl\", snli=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process to 0th sentences.\n",
      "Process to 1th sentences.\n",
      "Process to 2th sentences.\n",
      "Process to 3th sentences.\n",
      "Process to 4th sentences.\n",
      "Process to 5th sentences.\n",
      "Process to 6th sentences.\n",
      "Process to 7th sentences.\n",
      "Process to 8th sentences.\n",
      "Process to 9th sentences.\n",
      "Process to 10th sentences.\n",
      "Process to 11th sentences.\n",
      "Process to 12th sentences.\n",
      "Process to 13th sentences.\n",
      "Process to 14th sentences.\n",
      "Process to 15th sentences.\n",
      "Process to 16th sentences.\n",
      "Process to 17th sentences.\n",
      "Process to 18th sentences.\n",
      "Process to 19th sentences.\n",
      "Process to 20th sentences.\n",
      "Process to 21th sentences.\n",
      "Process to 22th sentences.\n",
      "Process to 23th sentences.\n",
      "Process to 24th sentences.\n",
      "Process to 25th sentences.\n",
      "Process to 26th sentences.\n",
      "Process to 27th sentences.\n",
      "Process to 28th sentences.\n",
      "Process to 29th sentences.\n",
      "Process to 30th sentences.\n",
      "Process to 31th sentences.\n",
      "Process to 32th sentences.\n",
      "Process to 33th sentences.\n",
      "Process to 34th sentences.\n",
      "Process to 35th sentences.\n",
      "Process to 36th sentences.\n",
      "Process to 37th sentences.\n",
      "Process to 38th sentences.\n",
      "Process to 39th sentences.\n",
      "Process to 40th sentences.\n",
      "Process to 41th sentences.\n",
      "Process to 42th sentences.\n",
      "Process to 43th sentences.\n",
      "Process to 44th sentences.\n",
      "Process to 45th sentences.\n",
      "Process to 46th sentences.\n",
      "Process to 47th sentences.\n",
      "Process to 48th sentences.\n",
      "Process to 49th sentences.\n",
      "Process to 50th sentences.\n",
      "Process to 51th sentences.\n",
      "Process to 52th sentences.\n",
      "Process to 53th sentences.\n",
      "Process to 54th sentences.\n",
      "Process to 55th sentences.\n",
      "Process to 56th sentences.\n",
      "Process to 57th sentences.\n",
      "Process to 58th sentences.\n",
      "Process to 59th sentences.\n",
      "Process to 60th sentences.\n",
      "Process to 61th sentences.\n",
      "Process to 62th sentences.\n",
      "Process to 63th sentences.\n",
      "Process to 64th sentences.\n",
      "Process to 65th sentences.\n",
      "Process to 66th sentences.\n",
      "Process to 67th sentences.\n",
      "Process to 68th sentences.\n",
      "Process to 69th sentences.\n",
      "Process to 70th sentences.\n",
      "Process to 71th sentences.\n",
      "Process to 72th sentences.\n",
      "Process to 73th sentences.\n",
      "Process to 74th sentences.\n",
      "Process to 75th sentences.\n",
      "Process to 76th sentences.\n",
      "Process to 77th sentences.\n",
      "Process to 78th sentences.\n",
      "Process to 79th sentences.\n",
      "Process to 80th sentences.\n",
      "Process to 81th sentences.\n",
      "Process to 82th sentences.\n",
      "Process to 83th sentences.\n",
      "Process to 84th sentences.\n",
      "Process to 85th sentences.\n",
      "Process to 86th sentences.\n",
      "Process to 87th sentences.\n",
      "Process to 88th sentences.\n",
      "Process to 89th sentences.\n",
      "Process to 90th sentences.\n",
      "Process to 91th sentences.\n",
      "Process to 92th sentences.\n",
      "Process to 93th sentences.\n",
      "Process to 94th sentences.\n",
      "Process to 95th sentences.\n",
      "Process to 96th sentences.\n",
      "Process to 97th sentences.\n",
      "Process to 98th sentences.\n",
      "Process to 99th sentences.\n",
      "Process to 100th sentences.\n",
      "Process to 101th sentences.\n",
      "Process to 102th sentences.\n",
      "Process to 103th sentences.\n",
      "Process to 104th sentences.\n",
      "Process to 105th sentences.\n",
      "Process to 106th sentences.\n",
      "Process to 107th sentences.\n",
      "Process to 108th sentences.\n",
      "Process to 109th sentences.\n",
      "Process to 110th sentences.\n",
      "Process to 111th sentences.\n",
      "Process to 112th sentences.\n",
      "Process to 113th sentences.\n",
      "Process to 114th sentences.\n",
      "Process to 115th sentences.\n",
      "Process to 116th sentences.\n",
      "Process to 117th sentences.\n",
      "Process to 118th sentences.\n",
      "Process to 119th sentences.\n",
      "Process to 120th sentences.\n",
      "Process to 121th sentences.\n",
      "Process to 122th sentences.\n",
      "Process to 123th sentences.\n",
      "Process to 124th sentences.\n",
      "Process to 125th sentences.\n",
      "Process to 126th sentences.\n",
      "Process to 127th sentences.\n",
      "Process to 128th sentences.\n",
      "Process to 129th sentences.\n",
      "Process to 130th sentences.\n",
      "Process to 131th sentences.\n",
      "Process to 132th sentences.\n",
      "Process to 133th sentences.\n",
      "Process to 134th sentences.\n",
      "Process to 135th sentences.\n",
      "Process to 136th sentences.\n",
      "Process to 137th sentences.\n",
      "Process to 138th sentences.\n",
      "Process to 139th sentences.\n",
      "Process to 140th sentences.\n",
      "Process to 141th sentences.\n",
      "Process to 142th sentences.\n",
      "Process to 143th sentences.\n",
      "Process to 144th sentences.\n",
      "Process to 145th sentences.\n",
      "Process to 146th sentences.\n",
      "Process to 147th sentences.\n",
      "Process to 148th sentences.\n",
      "Process to 149th sentences.\n",
      "Process to 150th sentences.\n",
      "Process to 151th sentences.\n",
      "Process to 152th sentences.\n",
      "Process to 153th sentences.\n",
      "Process to 154th sentences.\n",
      "Process to 155th sentences.\n",
      "Process to 156th sentences.\n",
      "Process to 157th sentences.\n",
      "Process to 158th sentences.\n",
      "Process to 159th sentences.\n",
      "Process to 160th sentences.\n",
      "Process to 161th sentences.\n",
      "Process to 162th sentences.\n",
      "Process to 163th sentences.\n",
      "Process to 164th sentences.\n",
      "Process to 165th sentences.\n",
      "Process to 166th sentences.\n",
      "Process to 167th sentences.\n",
      "Process to 168th sentences.\n",
      "Process to 169th sentences.\n",
      "Process to 170th sentences.\n",
      "Process to 171th sentences.\n",
      "Process to 172th sentences.\n",
      "Process to 173th sentences.\n",
      "Process to 174th sentences.\n",
      "Process to 175th sentences.\n",
      "Process to 176th sentences.\n",
      "Process to 177th sentences.\n",
      "Process to 178th sentences.\n",
      "Process to 179th sentences.\n",
      "Process to 180th sentences.\n",
      "Process to 181th sentences.\n",
      "Process to 182th sentences.\n",
      "Process to 183th sentences.\n",
      "Process to 184th sentences.\n",
      "Process to 185th sentences.\n",
      "Process to 186th sentences.\n",
      "Process to 187th sentences.\n",
      "Process to 188th sentences.\n",
      "Process to 189th sentences.\n",
      "Process to 190th sentences.\n",
      "Process to 191th sentences.\n",
      "Process to 192th sentences.\n",
      "Process to 193th sentences.\n",
      "Process to 194th sentences.\n",
      "Process to 195th sentences.\n"
     ]
    }
   ],
   "source": [
    "def relation_feat(WORD1, WORD2):\n",
    "    # init\n",
    "    relation_feat_syn = None\n",
    "    relation_feat_ant = None\n",
    "    relation_feat_hype = None\n",
    "    relation_feat_hypo = None\n",
    "    relation_feat_same_hype = None\n",
    "    \n",
    "    syn_flg = False\n",
    "    WORD1_syn_lst = wn.synsets(WORD1)\n",
    "    WORD2_syn_lst =wn.synsets(WORD2)\n",
    "    if not WORD1_syn_lst or not WORD2_syn_lst:\n",
    "        return [0,0,0,0,0]\n",
    "    \n",
    "    # synonymy\n",
    "    for word1_i, word1_syn in enumerate(WORD1_syn_lst):\n",
    "        for word2_i, word2_syn in enumerate(WORD2_syn_lst):\n",
    "            if word1_syn == word2_syn:\n",
    "                relation_feat_syn = 1\n",
    "\n",
    "    # antonymy\n",
    "    syn_flg = True\n",
    "    for word1_i, word1_syn in enumerate(WORD1_syn_lst):\n",
    "        for word2_i, word2_syn in enumerate(WORD2_syn_lst):\n",
    "            if word1_syn.lemmas()[0].antonyms():\n",
    "                if word1_syn.lemmas()[0].antonyms()[0] == word2_syn.lemmas()[0]:\n",
    "                    relation_feat_ant = 1\n",
    "    if not relation_feat_syn:\n",
    "        relation_feat_syn = 0\n",
    "    if not relation_feat_ant:\n",
    "        relation_feat_ant = 0\n",
    "        \n",
    "    # hypernymy&hyponymy\n",
    "    ## first find their most similarity def\n",
    "    similarity = -np.float('inf')\n",
    "    sim_flag = False\n",
    "    for word1_i, word1_syn in enumerate(WORD1_syn_lst):\n",
    "        for word2_i, word2_syn in enumerate(WORD2_syn_lst):\n",
    "            sim_val = wn.wup_similarity(word1_syn, word2_syn)\n",
    "            if sim_val: # if they exist similaroty value\n",
    "                sim_flag = True\n",
    "                if sim_val > similarity:\n",
    "                    best_val = sim_val\n",
    "                    best_syn1 = word1_syn\n",
    "                    best_syn2 = word2_syn\n",
    "                    similarity = sim_val\n",
    "    if not sim_flag: # in case the 2 words don't exist any similarity definision synset.\n",
    "        return [0,0,0,0,0]\n",
    "    \n",
    "    ## base on the def, to figure out their hyper-path\n",
    "    best_syn1_path = best_syn1.hypernym_paths()\n",
    "    for idx, path in enumerate(best_syn1_path):\n",
    "        for i, hyper in enumerate(path):\n",
    "            if hyper == best_syn2:\n",
    "                edgN = len(path)-i-1\n",
    "                relation_feat_hype = 1-edgN/8\n",
    "                break\n",
    "        if relation_feat_hype:\n",
    "            break\n",
    "\n",
    "    best_syn2_path = best_syn2.hypernym_paths()\n",
    "    for idx, path in enumerate(best_syn2_path):\n",
    "        for i, hyper in enumerate(path):\n",
    "            if hyper == best_syn1:\n",
    "                edgN = len(path)-i-1\n",
    "                relation_feat_hypo = 1-edgN/8\n",
    "                break\n",
    "        if relation_feat_hypo:\n",
    "            break\n",
    "\n",
    "    if relation_feat_hype:\n",
    "        relation_feat_hypo = relation_feat_hype\n",
    "    elif relation_feat_hypo:\n",
    "        relation_feat_hype = relation_feat_hypo\n",
    "    else:\n",
    "        relation_feat_hype = 0\n",
    "        relation_feat_hypo = 0\n",
    "\n",
    "    ## Same-Hypernym\n",
    "    if relation_feat_syn == 0:\n",
    "        # to see if the two words have the same hypernym\n",
    "        for syn1_idx, syn1_path in enumerate(best_syn1_path):\n",
    "            for syn2_idx, syn2_path in enumerate(best_syn2_path):\n",
    "                for syn1_hyper in syn1_path:\n",
    "                    for syn2_hyper in syn2_path:\n",
    "                        if syn1_hyper == syn2_hyper:\n",
    "    #                         print(syn1_hyper)\n",
    "    #                         print(syn2_hyper)\n",
    "                            relation_feat_same_hype = 1\n",
    "    if not relation_feat_same_hype:\n",
    "        relation_feat_same_hype = 0\n",
    "        \n",
    "    return [relation_feat_syn, relation_feat_ant, relation_feat_hype, relation_feat_hypo, relation_feat_same_hype]\n",
    "\n",
    "# remove all punctuations (except while space)\n",
    "def preprocess_word(str_word):\n",
    "    signtext = string.punctuation \n",
    "    signrepl = '@'*len(signtext)\n",
    "    signtable = str_word.maketrans(signtext,signrepl) \n",
    "    return str_word.translate(signtable).replace('@','')\n",
    "\n",
    "Dic_external_know = {}\n",
    "for data_i, data_ in enumerate(test_snli):\n",
    "    print('Process to {}th sentences.'.format(data_i))\n",
    "    sent_1 = data_['sentence1']\n",
    "    sent_2 = data_['sentence2'] \n",
    "    sent_1 = preprocess_word(sent_1) # remove all punctuations\n",
    "    sent_2 = preprocess_word(sent_2) # remove all punctuations\n",
    "    for word1 in sent_1.split():\n",
    "        for word2 in sent_2.split():\n",
    "            R_lst = relation_feat(word1, word2) # five relation feature value: [ayn, ant, hype, hypo, same-hype]\n",
    "            if all(ele == 0 for ele in R_lst):\n",
    "                l_R = 0 # lr_ij\n",
    "            else:\n",
    "                l_R = 1 # lr_ij\n",
    "            Dic_external_know[(word1,word2)] = l_R # key: (word1, word2); value: L(R_ij) [should be 1 or 0]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank.\n",
      "bank\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "output = open('snli_test_relationship_feat.pkl', 'wb')\n",
    "pickle.dump(Dic_external_know, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'A'): 1,\n",
       " ('A', 'beach'): 1,\n",
       " ('A', 'giant'): 1,\n",
       " ('A', 'splashes'): 1,\n",
       " ('A', 'the'): 0,\n",
       " ('A', 'wave'): 1,\n",
       " ('Dangerous', 'A'): 0,\n",
       " ('Dangerous', 'beach'): 0,\n",
       " ('Dangerous', 'giant'): 0,\n",
       " ('Dangerous', 'splashes'): 0,\n",
       " ('Dangerous', 'the'): 0,\n",
       " ('Dangerous', 'wave'): 0,\n",
       " ('Shorebreak', 'A'): 0,\n",
       " ('Shorebreak', 'beach'): 0,\n",
       " ('Shorebreak', 'giant'): 0,\n",
       " ('Shorebreak', 'splashes'): 0,\n",
       " ('Shorebreak', 'the'): 0,\n",
       " ('Shorebreak', 'wave'): 0,\n",
       " ('an', 'A'): 1,\n",
       " ('an', 'beach'): 1,\n",
       " ('an', 'giant'): 1,\n",
       " ('an', 'splashes'): 1,\n",
       " ('an', 'the'): 0,\n",
       " ('an', 'wave'): 1,\n",
       " ('beach', 'A'): 1,\n",
       " ('beach', 'beach'): 1,\n",
       " ('beach', 'giant'): 1,\n",
       " ('beach', 'splashes'): 1,\n",
       " ('beach', 'the'): 0,\n",
       " ('beach', 'wave'): 1,\n",
       " ('day', 'A'): 1,\n",
       " ('day', 'beach'): 1,\n",
       " ('day', 'giant'): 1,\n",
       " ('day', 'splashes'): 1,\n",
       " ('day', 'the'): 0,\n",
       " ('day', 'wave'): 1,\n",
       " ('hang', 'A'): 1,\n",
       " ('hang', 'beach'): 0,\n",
       " ('hang', 'giant'): 1,\n",
       " ('hang', 'splashes'): 1,\n",
       " ('hang', 'the'): 0,\n",
       " ('hang', 'wave'): 1,\n",
       " ('indicates', 'A'): 0,\n",
       " ('indicates', 'beach'): 0,\n",
       " ('indicates', 'giant'): 0,\n",
       " ('indicates', 'splashes'): 0,\n",
       " ('indicates', 'the'): 0,\n",
       " ('indicates', 'wave'): 1,\n",
       " ('on', 'A'): 0,\n",
       " ('on', 'beach'): 0,\n",
       " ('on', 'giant'): 0,\n",
       " ('on', 'splashes'): 0,\n",
       " ('on', 'the'): 0,\n",
       " ('on', 'wave'): 0,\n",
       " ('out', 'A'): 1,\n",
       " ('out', 'beach'): 0,\n",
       " ('out', 'giant'): 0,\n",
       " ('out', 'splashes'): 1,\n",
       " ('out', 'the'): 0,\n",
       " ('out', 'wave'): 1,\n",
       " ('overcast', 'A'): 1,\n",
       " ('overcast', 'beach'): 1,\n",
       " ('overcast', 'giant'): 1,\n",
       " ('overcast', 'splashes'): 1,\n",
       " ('overcast', 'the'): 0,\n",
       " ('overcast', 'wave'): 1,\n",
       " ('people', 'A'): 1,\n",
       " ('people', 'beach'): 0,\n",
       " ('people', 'giant'): 1,\n",
       " ('people', 'splashes'): 1,\n",
       " ('people', 'the'): 0,\n",
       " ('people', 'wave'): 1,\n",
       " ('several', 'A'): 0,\n",
       " ('several', 'beach'): 0,\n",
       " ('several', 'giant'): 0,\n",
       " ('several', 'splashes'): 0,\n",
       " ('several', 'the'): 0,\n",
       " ('several', 'wave'): 0,\n",
       " ('sign', 'A'): 1,\n",
       " ('sign', 'beach'): 1,\n",
       " ('sign', 'giant'): 1,\n",
       " ('sign', 'splashes'): 1,\n",
       " ('sign', 'the'): 0,\n",
       " ('sign', 'wave'): 1,\n",
       " ('the', 'A'): 0,\n",
       " ('the', 'beach'): 0,\n",
       " ('the', 'giant'): 0,\n",
       " ('the', 'splashes'): 0,\n",
       " ('the', 'the'): 0,\n",
       " ('the', 'wave'): 0,\n",
       " ('while', 'A'): 1,\n",
       " ('while', 'beach'): 1,\n",
       " ('while', 'giant'): 1,\n",
       " ('while', 'splashes'): 1,\n",
       " ('while', 'the'): 0,\n",
       " ('while', 'wave'): 1}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dic_external_know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
